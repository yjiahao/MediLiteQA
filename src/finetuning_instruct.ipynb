{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Install Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-03T10:30:00.249614Z",
     "iopub.status.busy": "2025-11-03T10:30:00.249366Z",
     "iopub.status.idle": "2025-11-03T10:31:53.584899Z",
     "shell.execute_reply": "2025-11-03T10:31:53.583810Z",
     "shell.execute_reply.started": "2025-11-03T10:30:00.249595Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.0/44.0 kB\u001b[0m \u001b[31m1.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m423.1/423.1 kB\u001b[0m \u001b[31m10.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m106.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m0:01\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m78.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m43.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m31.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m13.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m86.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m47.7/47.7 MB\u001b[0m \u001b[31m37.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.0/12.0 MB\u001b[0m \u001b[31m99.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m0:01\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m566.1/566.1 kB\u001b[0m \u001b[31m30.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.3/3.3 MB\u001b[0m \u001b[31m83.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
      "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "bigframes 2.12.0 requires google-cloud-bigquery-storage<3.0.0,>=2.30.0, which is not installed.\n",
      "pylibcudf-cu12 25.2.2 requires pyarrow<20.0.0a0,>=14.0.0; platform_machine == \"x86_64\", but you have pyarrow 22.0.0 which is incompatible.\n",
      "cudf-cu12 25.2.2 requires pyarrow<20.0.0a0,>=14.0.0; platform_machine == \"x86_64\", but you have pyarrow 22.0.0 which is incompatible.\n",
      "bigframes 2.12.0 requires google-cloud-bigquery[bqstorage,pandas]>=3.31.0, but you have google-cloud-bigquery 3.25.0 which is incompatible.\n",
      "bigframes 2.12.0 requires rich<14,>=12.4.4, but you have rich 14.1.0 which is incompatible.\n",
      "libcugraph-cu12 25.6.0 requires libraft-cu12==25.6.*, but you have libraft-cu12 25.2.0 which is incompatible.\n",
      "gradio 5.38.1 requires pydantic<2.12,>=2.0, but you have pydantic 2.12.0a1 which is incompatible.\n",
      "cudf-polars-cu12 25.6.0 requires pylibcudf-cu12==25.6.*, but you have pylibcudf-cu12 25.2.2 which is incompatible.\n",
      "pandas-gbq 0.29.2 requires google-api-core<3.0.0,>=2.10.2, but you have google-api-core 1.34.1 which is incompatible.\n",
      "pylibcugraph-cu12 25.6.0 requires pylibraft-cu12==25.6.*, but you have pylibraft-cu12 25.2.0 which is incompatible.\n",
      "pylibcugraph-cu12 25.6.0 requires rmm-cu12==25.6.*, but you have rmm-cu12 25.2.0 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m68.6/68.6 kB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m864.4/864.4 kB\u001b[0m \u001b[31m15.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m63.5/63.5 MB\u001b[0m \u001b[31m27.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m325.4/325.4 kB\u001b[0m \u001b[31m16.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m444.8/444.8 kB\u001b[0m \u001b[31m26.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m56.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
      "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "thinc 8.3.6 requires numpy<3.0.0,>=2.0.0, but you have numpy 1.26.4 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m59.4/59.4 MB\u001b[0m \u001b[31m29.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25h"
     ]
    }
   ],
   "source": [
    "!pip install \"trl>=0.7.0\" \"datasets>=2.14.0\" \"torch>=2.0.0\" --quiet\n",
    "!pip install \"accelerate>=0.24.0\" \"peft>=0.7.0\" \"trackio==0.5.0\" --quiet\n",
    "!pip install bitsandbytes --quiet\n",
    "!pip install transformers --quiet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-03T10:31:53.586940Z",
     "iopub.status.busy": "2025-11-03T10:31:53.586648Z",
     "iopub.status.idle": "2025-11-03T10:32:27.937710Z",
     "shell.execute_reply": "2025-11-03T10:32:27.936926Z",
     "shell.execute_reply.started": "2025-11-03T10:31:53.586911Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-03 10:32:04.527228: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1762165924.738518      57 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1762165924.807156      57 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n"
     ]
    }
   ],
   "source": [
    "from kagglehub import model_download, KaggleDatasetAdapter, load_dataset\n",
    "import os\n",
    "from peft import LoraConfig, PeftModel\n",
    "import torch\n",
    "import trackio\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig, set_seed, TrainingArguments\n",
    "from trl import SFTTrainer, SFTConfig\n",
    "\n",
    "from kaggle_secrets import UserSecretsClient\n",
    "secret_label = \"HF_TOKEN\" # your huggingface token label in a Kaggle secret\n",
    "HF_TOKEN = UserSecretsClient().get_secret(secret_label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set Seed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-03T10:32:27.938792Z",
     "iopub.status.busy": "2025-11-03T10:32:27.938530Z",
     "iopub.status.idle": "2025-11-03T10:32:27.949364Z",
     "shell.execute_reply": "2025-11-03T10:32:27.948625Z",
     "shell.execute_reply.started": "2025-11-03T10:32:27.938767Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "SEED = 42\n",
    "\n",
    "# Set seed for reproducibility\n",
    "set_seed(SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Choose Device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-03T10:32:27.951039Z",
     "iopub.status.busy": "2025-11-03T10:32:27.950809Z",
     "iopub.status.idle": "2025-11-03T10:32:27.967354Z",
     "shell.execute_reply": "2025-11-03T10:32:27.966627Z",
     "shell.execute_reply.started": "2025-11-03T10:32:27.951018Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using CUDA GPU: Tesla P100-PCIE-16GB\n",
      "GPU memory: 17.1GB\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "    device = \"cuda\"\n",
    "    print(f\"Using CUDA GPU: {torch.cuda.get_device_name()}\")\n",
    "    print(f\"GPU memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f}GB\")\n",
    "\n",
    "else:\n",
    "    device = \"cpu\"\n",
    "    print(\"Using CPU - you will need to use a GPU to train models\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-03T10:32:27.968326Z",
     "iopub.status.busy": "2025-11-03T10:32:27.968081Z",
     "iopub.status.idle": "2025-11-03T10:32:28.809014Z",
     "shell.execute_reply": "2025-11-03T10:32:28.808290Z",
     "shell.execute_reply.started": "2025-11-03T10:32:27.968307Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_57/2284682056.py:4: DeprecationWarning: Use dataset_load() instead of load_dataset(). load_dataset() will be removed in a future version.\n",
      "  medquad_train = load_dataset(\n",
      "/tmp/ipykernel_57/2284682056.py:10: DeprecationWarning: Use dataset_load() instead of load_dataset(). load_dataset() will be removed in a future version.\n",
      "  medquad_val = load_dataset(\n",
      "/tmp/ipykernel_57/2284682056.py:16: DeprecationWarning: Use dataset_load() instead of load_dataset(). load_dataset() will be removed in a future version.\n",
      "  medquad_test = load_dataset(\n"
     ]
    }
   ],
   "source": [
    "DATASET_PATH = \"bingxuanchia/dsa4213-medquad-processed-dataset\"\n",
    "\n",
    "# Load and verify dataset\n",
    "medquad_train = load_dataset(\n",
    "    KaggleDatasetAdapter.HUGGING_FACE,\n",
    "    DATASET_PATH,\n",
    "    \"train.csv\",\n",
    ")\n",
    "\n",
    "medquad_val = load_dataset(\n",
    "    KaggleDatasetAdapter.HUGGING_FACE,\n",
    "    DATASET_PATH,\n",
    "    \"val.csv\",\n",
    ")\n",
    "\n",
    "medquad_test = load_dataset(\n",
    "    KaggleDatasetAdapter.HUGGING_FACE,\n",
    "    DATASET_PATH,\n",
    "    \"test.csv\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Format Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Load pre-processed training set from Kaggle.\n",
    "- Convert training set into a conversational format. This is a list of message pairs, where each pair consists of a question from the user and an answer from the assistant."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-03T10:32:28.809936Z",
     "iopub.status.busy": "2025-11-03T10:32:28.809736Z",
     "iopub.status.idle": "2025-11-03T10:32:28.814706Z",
     "shell.execute_reply": "2025-11-03T10:32:28.813931Z",
     "shell.execute_reply.started": "2025-11-03T10:32:28.809920Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def format_qa_dataset(example, question_col, answer_col):\n",
    "    \"\"\"Formats QA datasets into chat format\"\"\"\n",
    "    formatted_message_pairs = []\n",
    "    \n",
    "    for question, answer in zip(example[question_col], example[answer_col]):\n",
    "        messages = [\n",
    "            {\"role\": \"user\", \"content\": question},\n",
    "            {\"role\": \"assistant\", \"content\": answer}\n",
    "        ]\n",
    "        formatted_message_pairs.append(messages)\n",
    "    \n",
    "    return {\"messages\": formatted_message_pairs}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-03T10:32:28.816019Z",
     "iopub.status.busy": "2025-11-03T10:32:28.815699Z",
     "iopub.status.idle": "2025-11-03T10:32:28.999258Z",
     "shell.execute_reply": "2025-11-03T10:32:28.998448Z",
     "shell.execute_reply.started": "2025-11-03T10:32:28.815995Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7c697984f1bc4286b058fdf3c02b756d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/11156 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "16c04715df3d4db8a3f8350c6d7cb114",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1394 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "84170ddbbaf04803ae7cf9c84dffb343",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1396 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "medquad_train_formatted = medquad_train.map(\n",
    "    format_qa_dataset,\n",
    "    batched=True,\n",
    "    fn_kwargs={\"question_col\": \"question\", \"answer_col\": \"answer\"},\n",
    "    remove_columns=medquad_train.column_names\n",
    ")\n",
    "\n",
    "medquad_val_formatted = medquad_val.map(\n",
    "    format_qa_dataset,\n",
    "    batched=True,\n",
    "    fn_kwargs={\"question_col\": \"question\", \"answer_col\": \"answer\"},\n",
    "    remove_columns=medquad_val.column_names\n",
    ")\n",
    "\n",
    "medquad_test_formatted = medquad_test.map(\n",
    "    format_qa_dataset,\n",
    "    batched=True,\n",
    "    fn_kwargs={\"question_col\": \"question\", \"answer_col\": \"answer\"},\n",
    "    remove_columns=medquad_val.column_names\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-03T10:32:29.000325Z",
     "iopub.status.busy": "2025-11-03T10:32:29.000046Z",
     "iopub.status.idle": "2025-11-03T10:32:29.006863Z",
     "shell.execute_reply": "2025-11-03T10:32:29.006138Z",
     "shell.execute_reply.started": "2025-11-03T10:32:29.000301Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'content': 'What are the treatments for Refsum Disease ?', 'role': 'user'},\n",
       " {'content': 'The primary treatment for ARD is to restrict or avoid foods that contain phytanic acid, including dairy products; beef and lamb; and fatty fish such as tuna, cod, and haddock. Some individuals may also require plasma exchange (plasmapheresis) in which blood is drawn, filtered, and reinfused back into the body, to control the buildup of phytanic acid.',\n",
       "  'role': 'assistant'}]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 'messages' here are formatted QA pairs of user and \n",
    "# assistant messages for SFT\n",
    "medquad_train_formatted[\"messages\"][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fine-Tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Select Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-03T10:32:29.007822Z",
     "iopub.status.busy": "2025-11-03T10:32:29.007575Z",
     "iopub.status.idle": "2025-11-03T10:32:29.019423Z",
     "shell.execute_reply": "2025-11-03T10:32:29.018631Z",
     "shell.execute_reply.started": "2025-11-03T10:32:29.007801Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Choose base model\n",
    "# model_name = \"HuggingFaceTB/SmolLM2-1.7B\"\n",
    "\n",
    "# Choose instruct model which will be used to make ChatTemplate\n",
    "instruct_model_name = \"HuggingFaceTB/SmolLM2-1.7B-Instruct\"\n",
    "\n",
    "# Set name of finetuned model\n",
    "finetuned_model_name = \"SmolLM2-1.7B-Instruct-MediLite-QA-Rank8-Quantized-HighLR\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training Configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-03T10:32:29.022165Z",
     "iopub.status.busy": "2025-11-03T10:32:29.021940Z",
     "iopub.status.idle": "2025-11-03T10:32:29.034185Z",
     "shell.execute_reply": "2025-11-03T10:32:29.033482Z",
     "shell.execute_reply.started": "2025-11-03T10:32:29.022150Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Path which will store the saved weights\n",
    "MODEL_WEIGHTS_PATH = \"weights\"\n",
    "os.makedirs(MODEL_WEIGHTS_PATH, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-03T10:32:29.035364Z",
     "iopub.status.busy": "2025-11-03T10:32:29.035089Z",
     "iopub.status.idle": "2025-11-03T10:32:29.047404Z",
     "shell.execute_reply": "2025-11-03T10:32:29.046693Z",
     "shell.execute_reply.started": "2025-11-03T10:32:29.035343Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# To conduct experiment tracking with trackio\n",
    "# Dashboard viewable on Hugging Face Spaces\n",
    "os.environ[\"HF_TOKEN\"] = HF_TOKEN\n",
    "os.environ[\"TRACKIO_SPACE_ID\"] = \"Jiahao123/MediLiteQA\"\n",
    "os.environ[\"TRACKIO_PROJECT\"] = \"medilite-finetuning\"\n",
    "\n",
    "HF_REPO_ID = f\"Jiahao123/{finetuned_model_name}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-03T10:32:29.048201Z",
     "iopub.status.busy": "2025-11-03T10:32:29.048023Z",
     "iopub.status.idle": "2025-11-03T10:32:29.062481Z",
     "shell.execute_reply": "2025-11-03T10:32:29.061730Z",
     "shell.execute_reply.started": "2025-11-03T10:32:29.048187Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Quantization configurations\n",
    "quantization_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-02T04:18:01.471093Z",
     "iopub.status.busy": "2025-11-02T04:18:01.470841Z",
     "iopub.status.idle": "2025-11-02T04:18:01.642929Z",
     "shell.execute_reply": "2025-11-02T04:18:01.642139Z",
     "shell.execute_reply.started": "2025-11-02T04:18:01.471078Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Configure training parameters\n",
    "training_config = SFTConfig(\n",
    "    # Model and data\n",
    "    output_dir=os.path.join(MODEL_WEIGHTS_PATH, finetuned_model_name),\n",
    "    dataset_text_field=\"messages\",\n",
    "    max_length=2048,\n",
    "    chat_template_path=instruct_model_name,\n",
    "    \n",
    "    # Training hyperparameters\n",
    "    per_device_train_batch_size=4,  # Adjust based on GPU memory\n",
    "    gradient_accumulation_steps=8,\n",
    "    learning_rate=5e-5, # set higher learning rate\n",
    "    num_train_epochs=5,\n",
    "    seed=SEED,\n",
    "    \n",
    "    # Optimization\n",
    "    warmup_steps=50,\n",
    "    weight_decay=0.01,\n",
    "    optim=\"adamw_torch\",\n",
    "    \n",
    "    # Logging and saving\n",
    "    logging_strategy=\"steps\",\n",
    "    logging_steps=10,\n",
    "    save_strategy=\"epoch\",\n",
    "    save_total_limit=2,\n",
    "    eval_strategy=\"epoch\",\n",
    "    load_best_model_at_end=True,\n",
    "    \n",
    "    # Memory optimization\n",
    "    dataloader_num_workers=0,\n",
    "    group_by_length=True,  # Group similar length sequences\n",
    "\n",
    "    # Hugging Face Hub integration\n",
    "    push_to_hub=True,  # Set to True to upload to Hub\n",
    "    hub_model_id=HF_REPO_ID,\n",
    "\n",
    "    # Experiment tracking\n",
    "    report_to=[\"trackio\"],\n",
    "    run_name=finetuned_model_name,   # Set run name\n",
    ")\n",
    "\n",
    "print(\"Training configuration set!\")\n",
    "print(f\"Effective batch size: {training_config.per_device_train_batch_size * training_config.gradient_accumulation_steps}\")\n",
    "\n",
    "print(\"Training configuration set!\")\n",
    "print(f\"Effective batch size: {training_config.per_device_train_batch_size * training_config.gradient_accumulation_steps}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-02T04:18:01.643947Z",
     "iopub.status.busy": "2025-11-02T04:18:01.643694Z",
     "iopub.status.idle": "2025-11-02T04:18:01.647977Z",
     "shell.execute_reply": "2025-11-02T04:18:01.647196Z",
     "shell.execute_reply.started": "2025-11-02T04:18:01.643929Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# LoRA configurations\n",
    "peft_config = LoraConfig(\n",
    "    r=16,\n",
    "    lora_alpha=16,\n",
    "    lora_dropout=0.1,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-02T04:18:01.649004Z",
     "iopub.status.busy": "2025-11-02T04:18:01.648689Z",
     "iopub.status.idle": "2025-11-02T04:18:31.763766Z",
     "shell.execute_reply": "2025-11-02T04:18:31.762946Z",
     "shell.execute_reply.started": "2025-11-02T04:18:01.648983Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Load base model\n",
    "print(f\"Loading {instruct_model_name}...\")\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    instruct_model_name,\n",
    "    quantization_config=quantization_config,\n",
    "    device_map=\"auto\",\n",
    "    trust_remote_code=True\n",
    ")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(instruct_model_name)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_size = \"right\"\n",
    "\n",
    "print(f\"Base model {instruct_model_name} loaded! Parameters: {model.num_parameters():,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-02T04:18:31.764938Z",
     "iopub.status.busy": "2025-11-02T04:18:31.764625Z",
     "iopub.status.idle": "2025-11-02T04:18:48.206868Z",
     "shell.execute_reply": "2025-11-02T04:18:48.206065Z",
     "shell.execute_reply.started": "2025-11-02T04:18:31.764915Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Create SFTTrainer with LoRA enabled\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    train_dataset=medquad_train_formatted,\n",
    "    eval_dataset=medquad_val_formatted,\n",
    "    args=training_config,\n",
    "    peft_config=peft_config\n",
    ")\n",
    "\n",
    "print(\"Number of trainable parameters after LoRA:\")\n",
    "trainer.model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting fine tuning...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The `TRACKIO_PROJECT` environment variable is deprecated and will be removed in a future version. Use TrainingArguments.project instead.\n",
      "The `TRACKIO_SPACE_ID` environment variable is deprecated and will be removed in a future version. Use TrainingArguments.trackio_space_id instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Trackio project initialized: medilite-finetuning\n",
      "* Trackio metrics will be synced to Hugging Face Dataset: Jiahao123/MediLiteQA-dataset\n",
      "* Found existing space: https://huggingface.co/spaces/Jiahao123/MediLiteQA\n",
      "* View dashboard by going to: https://Jiahao123-MediLiteQA.hf.space/\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"https://Jiahao123-MediLiteQA.hf.space/\" width=\"100%\" height=\"1000px\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Created new run: SmolLM2-1.7B-Instruct-MediLite-QA-Rank8-Quantized-HighLR\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/torch/_dynamo/eval_frame.py:745: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1745' max='1745' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1745/1745 10:57:44, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Entropy</th>\n",
       "      <th>Num Tokens</th>\n",
       "      <th>Mean Token Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1.223300</td>\n",
       "      <td>1.279015</td>\n",
       "      <td>1.256274</td>\n",
       "      <td>3172757.000000</td>\n",
       "      <td>0.696983</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1.049600</td>\n",
       "      <td>1.173237</td>\n",
       "      <td>1.137791</td>\n",
       "      <td>6345514.000000</td>\n",
       "      <td>0.718449</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.962600</td>\n",
       "      <td>1.148381</td>\n",
       "      <td>1.127664</td>\n",
       "      <td>9518271.000000</td>\n",
       "      <td>0.722560</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.877800</td>\n",
       "      <td>1.135756</td>\n",
       "      <td>1.116395</td>\n",
       "      <td>12691028.000000</td>\n",
       "      <td>0.725041</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.894800</td>\n",
       "      <td>1.133617</td>\n",
       "      <td>1.116278</td>\n",
       "      <td>15863785.000000</td>\n",
       "      <td>0.725572</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/torch/_dynamo/eval_frame.py:745: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "/usr/local/lib/python3.11/dist-packages/torch/_dynamo/eval_frame.py:745: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "/usr/local/lib/python3.11/dist-packages/torch/_dynamo/eval_frame.py:745: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "/usr/local/lib/python3.11/dist-packages/torch/_dynamo/eval_frame.py:745: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Run finished. Uploading logs to Trackio (please wait...)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=1745, training_loss=1.2323178169720495, metrics={'train_runtime': 39578.5132, 'train_samples_per_second': 1.409, 'train_steps_per_second': 0.044, 'total_flos': 1.54503750494208e+17, 'train_loss': 1.2323178169720495, 'epoch': 5.0})"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(\"Starting fine tuning...\")\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-28T13:35:40.113251Z",
     "iopub.status.busy": "2025-10-28T13:35:40.112550Z",
     "iopub.status.idle": "2025-10-28T13:35:45.562215Z",
     "shell.execute_reply": "2025-10-28T13:35:45.561460Z",
     "shell.execute_reply.started": "2025-10-28T13:35:40.113226Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "trainer.push_to_hub()\n",
    "tokenizer.push_to_hub(HF_REPO_ID)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Zip Outputs for Download"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-28T13:35:31.638738Z",
     "iopub.status.busy": "2025-10-28T13:35:31.638464Z",
     "iopub.status.idle": "2025-10-28T13:35:34.160795Z",
     "shell.execute_reply": "2025-10-28T13:35:34.160056Z",
     "shell.execute_reply.started": "2025-10-28T13:35:31.638718Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "!zip -r finetuning_outputs.zip /kaggle/working"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Determine Testing Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-03T10:32:29.063411Z",
     "iopub.status.busy": "2025-11-03T10:32:29.063176Z",
     "iopub.status.idle": "2025-11-03T10:32:29.079503Z",
     "shell.execute_reply": "2025-11-03T10:32:29.078768Z",
     "shell.execute_reply.started": "2025-11-03T10:32:29.063397Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Jiahao123/SmolLM2-1.7B-Instruct-MediLite-QA-Rank8-Quantized-HighLR'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "HF_REPO_ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-03T10:32:29.080509Z",
     "iopub.status.busy": "2025-11-03T10:32:29.080184Z",
     "iopub.status.idle": "2025-11-03T10:33:00.151450Z",
     "shell.execute_reply": "2025-11-03T10:33:00.150811Z",
     "shell.execute_reply.started": "2025-11-03T10:32:29.080488Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "625874c0da594ea396779b63007c7d41",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/908 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "88c434565e4c4f04ba45e1f9b0b424be",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/3.42G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "84cd368a11d34991beaf986e19357267",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/132 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7535cf79caf24b35b227584eeae6ebbd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4fdc151ffcbd4976bf2e86b040a8e1e7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a0df26833d134ab99f7b76ba77cc7f30",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cf77ecebb5694dc6a09ad3a1e4c2060c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "49e7e2dc8c554fb8932b8b18c906b9f4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/655 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2b7cc06c39814a5d806a034950ff5ed3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "adapter_config.json:   0%|          | 0.00/839 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "161f016dbb644aa19bf60f02faed4383",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "adapter_model.safetensors:   0%|          | 0.00/6.30M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    instruct_model_name,\n",
    "    quantization_config=quantization_config,\n",
    "    device_map=\"auto\",\n",
    "    trust_remote_code=True\n",
    ")\n",
    "\n",
    "# Tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(instruct_model_name)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = \"right\"\n",
    "\n",
    "# Load LoRA weights\n",
    "model = PeftModel.from_pretrained(model, HF_REPO_ID)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-03T10:33:00.152488Z",
     "iopub.status.busy": "2025-11-03T10:33:00.152285Z",
     "iopub.status.idle": "2025-11-03T10:33:16.393491Z",
     "shell.execute_reply": "2025-11-03T10:33:16.392635Z",
     "shell.execute_reply.started": "2025-11-03T10:33:00.152472Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "61de8f11da00413c9d030040c903389a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Tokenizing train dataset:   0%|          | 0/11156 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0e66e306bef64484813325dbe71495fe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Truncating train dataset:   0%|          | 0/11156 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "47885309a243441da90f167c732db718",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Tokenizing eval dataset:   0%|          | 0/1396 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6aa9c0cdb1f34df798133c256fc46724",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Truncating eval dataset:   0%|          | 0/1396 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
    "\n",
    "# Configure training parameters\n",
    "training_config = SFTConfig(\n",
    "    # Model and data\n",
    "    output_dir=os.path.join(MODEL_WEIGHTS_PATH, finetuned_model_name),\n",
    "    dataset_text_field=\"messages\",\n",
    "    max_length=2048,\n",
    "    chat_template_path=instruct_model_name,\n",
    "    \n",
    "    # Training hyperparameters\n",
    "    per_device_train_batch_size=4,  # Adjust based on GPU memory\n",
    "    gradient_accumulation_steps=8,\n",
    "    learning_rate=5e-5, # set higher learning rate\n",
    "    num_train_epochs=5,\n",
    "    seed=SEED,\n",
    "    \n",
    "    # Optimization\n",
    "    warmup_steps=50,\n",
    "    weight_decay=0.01,\n",
    "    optim=\"adamw_torch\",\n",
    "    \n",
    "    # Logging and saving\n",
    "    logging_strategy=\"steps\",\n",
    "    logging_steps=10,\n",
    "    save_strategy=\"epoch\",\n",
    "    save_total_limit=2,\n",
    "    eval_strategy=\"epoch\",\n",
    "    load_best_model_at_end=True,\n",
    "    \n",
    "    # Memory optimization\n",
    "    dataloader_num_workers=0,\n",
    "    group_by_length=True,  # Group similar length sequences\n",
    "\n",
    "    # Hugging Face Hub integration\n",
    "    push_to_hub=False,  # Set to True to upload to Hub\n",
    "    # hub_model_id=HF_REPO_ID,\n",
    "\n",
    "    # Experiment tracking\n",
    "    report_to=[\"none\"],\n",
    "    run_name=finetuned_model_name,   # Set run name\n",
    ")\n",
    "\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    train_dataset=medquad_train_formatted,\n",
    "    eval_dataset=medquad_test_formatted, # use test set as eval dataset\n",
    "    args=training_config,\n",
    "    # peft_config=peft_config\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-03T10:33:16.394585Z",
     "iopub.status.busy": "2025-11-03T10:33:16.394331Z",
     "iopub.status.idle": "2025-11-03T10:38:47.730384Z",
     "shell.execute_reply": "2025-11-03T10:38:47.729787Z",
     "shell.execute_reply.started": "2025-11-03T10:33:16.394558Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='175' max='175' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [175/175 05:16]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 1.1401904821395874,\n",
       " 'eval_model_preparation_time': 0.0024,\n",
       " 'eval_runtime': 331.11,\n",
       " 'eval_samples_per_second': 4.216,\n",
       " 'eval_steps_per_second': 0.529,\n",
       " 'eval_entropy': 1.1269219660758971,\n",
       " 'eval_num_tokens': 0.0,\n",
       " 'eval_mean_token_accuracy': 0.7233400535583496}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.evaluate()"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "datasetId": 8360906,
     "sourceId": 13578406,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31154,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
